{"version":3,"sources":["/Users/keval/projects/atlas/doc/src/Hook-Storm.md"],"names":["layoutProps","MDXLayout","MDXContent","_ref","components","props","Object","_Users_keval_projects_atlas_doc_node_modules_babel_preset_react_app_node_modules_babel_runtime_helpers_esm_objectWithoutProperties__WEBPACK_IMPORTED_MODULE_0__","_mdx_js_react__WEBPACK_IMPORTED_MODULE_2__","assign","mdxType","id","parentName","className","isMDXComponent"],"mappings":"wWAWMA,EAAc,GAGdC,EAAY,UACH,SAASC,EAATC,GAGZ,IAFDC,EAECD,EAFDC,WACGC,EACFC,OAAAC,EAAA,EAAAD,CAAAH,EAAA,gBACD,OAAOG,OAAAE,EAAA,EAAAF,CAACL,EAADK,OAAAG,OAAA,GAAeT,EAAiBK,EAAhC,CAAuCD,WAAYA,EAAYM,QAAQ,cAC5EJ,OAAAE,EAAA,EAAAF,CAAA,KAAQ,CACNK,GAAM,sCADR,sCAGAL,OAAAE,EAAA,EAAAF,CAAA,KAAQ,CACNK,GAAM,gBADR,gBAGAL,OAAAE,EAAA,EAAAF,CAAA,8QAG8BA,OAAAE,EAAA,EAAAF,CAAA,MAAIM,WAAW,KAAf,YAH9B,KAIAN,OAAAE,EAAA,EAAAF,CAAA,uIAEAA,OAAAE,EAAA,EAAAF,CAAA,0PAIAA,OAAAE,EAAA,EAAAF,CAAA,8DACAA,OAAAE,EAAA,EAAAF,CAAA,UACEA,OAAAE,EAAA,EAAAF,CAAA,MAAIM,WAAW,MAAf,iDACAN,OAAAE,EAAA,EAAAF,CAAA,MAAIM,WAAW,MAAf,iDAEFN,OAAAE,EAAA,EAAAF,CAAA,KAAQ,CACNK,GAAM,oBADR,oBAGAL,OAAAE,EAAA,EAAAF,CAAA,8MAGAA,OAAAE,EAAA,EAAAF,CAAA,oDACAA,OAAAE,EAAA,EAAAF,CAAA,UACEA,OAAAE,EAAA,EAAAF,CAAA,MAAIM,WAAW,MAAf,qKACAN,OAAAE,EAAA,EAAAF,CAAA,MAAIM,WAAW,MAAf,qLACAN,OAAAE,EAAA,EAAAF,CAAA,MAAIM,WAAW,MAAf,oEACAN,OAAAE,EAAA,EAAAF,CAAA,MAAIM,WAAW,MAAf,4FAEFN,OAAAE,EAAA,EAAAF,CAAA,qJAEAA,OAAAE,EAAA,EAAAF,CAAA,uIAEAA,OAAAE,EAAA,EAAAF,CAAA,KAAQ,CACNK,GAAM,oBADR,oBAGAL,OAAAE,EAAA,EAAAF,CAAA,oMAGAA,OAAAE,EAAA,EAAAF,CAAA,6PAGAA,OAAAE,EAAA,EAAAF,CAAA,KAAQ,CACNK,GAAM,eADR,eAGAL,OAAAE,EAAA,EAAAF,CAAA,0EACAA,OAAAE,EAAA,EAAAF,CAAA,UACEA,OAAAE,EAAA,EAAAF,CAAA,MAAIM,WAAW,MAAf,8GACAN,OAAAE,EAAA,EAAAF,CAAA,MAAIM,WAAW,MAAf,2GACAN,OAAAE,EAAA,EAAAF,CAAA,MAAIM,WAAW,MAAf,uFAEFN,OAAAE,EAAA,EAAAF,CAAA,KAAQ,CACNK,GAAM,gBADR,gBAGAL,OAAAE,EAAA,EAAAF,CAAA,6FACAA,OAAAE,EAAA,EAAAF,CAAA,UACEA,OAAAE,EAAA,EAAAF,CAAA,MAAIM,WAAW,MAAf,2DACAN,OAAAE,EAAA,EAAAF,CAAA,MAAIM,WAAW,MAAf,iDACAN,OAAAE,EAAA,EAAAF,CAAA,MAAIM,WAAW,MAAf,sHAEFN,OAAAE,EAAA,EAAAF,CAAA,0JAEAA,OAAAE,EAAA,EAAAF,CAAA,oFACAA,OAAAE,EAAA,EAAAF,CAAA,KAAQ,CACNK,GAAM,iBADR,iBAGAL,OAAAE,EAAA,EAAAF,CAAA,KAAQ,CACNK,GAAM,uBADR,uBAGAL,OAAAE,EAAA,EAAAF,CAAA,mFACCA,OAAAE,EAAA,EAAAF,CAAA,MAAIM,WAAW,KAAf,+BADD,QAEAN,OAAAE,EAAA,EAAAF,CAAA,WAAKA,OAAAE,EAAA,EAAAF,CAAA,OAAAA,OAAAG,OAAA,CAAMG,WAAW,OAAU,CAC5BC,UAAa,mBADZ,oGAILP,OAAAE,EAAA,EAAAF,CAAA,wLAEAA,OAAAE,EAAA,EAAAF,CAAA,8iBAOAA,OAAAE,EAAA,EAAAF,CAAA,WAAKA,OAAAE,EAAA,EAAAF,CAAA,OAAAA,OAAAG,OAAA,CAAMG,WAAW,OAAU,CAC5BC,UAAa,mBADZ,yCAILP,OAAAE,EAAA,EAAAF,CAAA,eAAUA,OAAAE,EAAA,EAAAF,CAAA,MAAIM,WAAW,KAAf,kCAAV,6CACAN,OAAAE,EAAA,EAAAF,CAAA,WAAKA,OAAAE,EAAA,EAAAF,CAAA,OAAAA,OAAAG,OAAA,CAAMG,WAAW,OAAU,CAC5BC,UAAa,mBADZ,0DAILP,OAAAE,EAAA,EAAAF,CAAA,sEACAA,OAAAE,EAAA,EAAAF,CAAA,2EACAA,OAAAE,EAAA,EAAAF,CAAA,WAAKA,OAAAE,EAAA,EAAAF,CAAA,OAAAA,OAAAG,OAAA,CAAMG,WAAW,OAAU,CAC5BC,UAAa,mBADZ,2WAUTX,EAAWY,gBAAiB","file":"static/js/hook-storm.61c56e9b.js","sourcesContent":["/* @jsx mdx */\n  import React from 'react'\n  import { mdx } from '@mdx-js/react'\n  /* @jsx mdx */\n\n\nconst makeShortcode = name => function MDXDefaultShortcode(props) {\n  console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\")\n  return <div {...props}/>\n};\n\nconst layoutProps = {\n  \n};\nconst MDXLayout = \"wrapper\"\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n    <h1 {...{\n      \"id\": \"apache-atlas-hook-for-apache-storm\"\n    }}>{`Apache Atlas Hook for Apache Storm`}</h1>\n    <h2 {...{\n      \"id\": \"introduction\"\n    }}>{`Introduction`}</h2>\n    <p>{`Apache Storm is a distributed real-time computation system. Storm makes it\neasy to reliably process unbounded streams of data, doing for real-time\nprocessing what Hadoop did for batch processing. The process is essentially\na DAG of nodes, which is called `}<em parentName=\"p\">{`topology`}</em>{`.`}</p>\n    <p>{`Apache Atlas is a metadata repository that enables end-to-end data lineage,\nsearch and associate business classification.`}</p>\n    <p>{`The goal of this integration is to push the operational topology\nmetadata along with the underlying data source(s), target(s), derivation\nprocesses and any available business context so Atlas can capture the\nlineage for this topology.`}</p>\n    <p>{`There are 2 parts in this process detailed below:`}</p>\n    <ul>\n      <li parentName=\"ul\">{`Data model to represent the concepts in Storm`}</li>\n      <li parentName=\"ul\">{`Storm Atlas Hook to update metadata in Atlas`}</li>\n    </ul>\n    <h2 {...{\n      \"id\": \"storm-data-model\"\n    }}>{`Storm Data Model`}</h2>\n    <p>{`A data model is represented as Types in Atlas. It contains the descriptions\nof various nodes in the topology graph, such as spouts and bolts and the\ncorresponding producer and consumer types.`}</p>\n    <p>{`The following types are added in Atlas.`}</p>\n    <ul>\n      <li parentName=\"ul\">{`storm_topology - represents the coarse-grained topology. A storm_topology derives from an Atlas Process type and hence can be used to inform Atlas about lineage.`}</li>\n      <li parentName=\"ul\">{`Following data sets are added - kafka_topic, jms_topic, hbase_table, hdfs_data_set. These all derive from an Atlas Dataset type and hence form the end points of a lineage graph.`}</li>\n      <li parentName=\"ul\">{`storm_spout - Data Producer having outputs, typically Kafka, JMS`}</li>\n      <li parentName=\"ul\">{`storm_bolt - Data Consumer having inputs and outputs, typically Hive, HBase, HDFS, etc.`}</li>\n    </ul>\n    <p>{`The Storm Atlas hook auto registers dependent models like the Hive data model\nif it finds that these are not known to the Atlas server.`}</p>\n    <p>{`The data model for each of the types is described in\nthe class definition at org.apache.atlas.storm.model.StormDataModel.`}</p>\n    <h2 {...{\n      \"id\": \"storm-atlas-hook\"\n    }}>{`Storm Atlas Hook`}</h2>\n    <p>{`Atlas is notified when a new topology is registered successfully in\nStorm. Storm provides a hook, backtype.storm.ISubmitterHook, at the Storm client used to\nsubmit a storm topology.`}</p>\n    <p>{`The Storm Atlas hook intercepts the hook post execution and extracts the metadata from the\ntopology and updates Atlas using the types defined. Atlas implements the\nStorm client hook interface in org.apache.atlas.storm.hook.StormAtlasHook.`}</p>\n    <h2 {...{\n      \"id\": \"limitations\"\n    }}>{`Limitations`}</h2>\n    <p>{`The following apply for the first version of the integration.`}</p>\n    <ul>\n      <li parentName=\"ul\">{`Only new topology submissions are registered with Atlas, any lifecycle changes are not reflected in Atlas.`}</li>\n      <li parentName=\"ul\">{`The Atlas server needs to be online when a Storm topology is submitted for the metadata to be captured.`}</li>\n      <li parentName=\"ul\">{`The Hook currently does not support capturing lineage for custom spouts and bolts.`}</li>\n    </ul>\n    <h2 {...{\n      \"id\": \"installation\"\n    }}>{`Installation`}</h2>\n    <p>{`The Storm Atlas Hook needs to be manually installed in Storm on the client side.`}</p>\n    <ul>\n      <li parentName=\"ul\">{`untar apache-atlas-\\${project.version}-storm-hook.tar.gz`}</li>\n      <li parentName=\"ul\">{`cd apache-atlas-storm-hook-\\${project.version}`}</li>\n      <li parentName=\"ul\">{`Copy entire contents of folder apache-atlas-storm-hook-\\${project.version}/hook/storm to $ATLAS_PACKAGE/hook/storm`}</li>\n    </ul>\n    <p>{`Storm Atlas hook jars in $ATLAS_PACKAGE/hook/storm need to be copied to $STORM_HOME/extlib.\nReplace STORM_HOME with storm installation path.`}</p>\n    <p>{`Restart all daemons after you have installed the atlas hook into Storm.`}</p>\n    <h2 {...{\n      \"id\": \"configuration\"\n    }}>{`Configuration`}</h2>\n    <h3 {...{\n      \"id\": \"storm-configuration\"\n    }}>{`Storm Configuration`}</h3>\n    <p>{`The Storm Atlas Hook needs to be configured in Storm client config\nin `}<em parentName=\"p\">{`$STORM_HOME/conf/storm.yaml`}</em>{` as:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`storm.topology.submission.notifier.plugin.class: \"org.apache.atlas.storm.hook.StormAtlasHook\"\n`}</code></pre>\n    <p>{`Also set a 'cluster name' that would be used as a namespace for objects registered in Atlas.\nThis name would be used for namespacing the Storm topology, spouts and bolts.`}</p>\n    <p>{`The other objects like data sets should ideally be identified with the cluster name of\nthe components that generate them. For e.g. Hive tables and databases should be\nidentified using the cluster name set in Hive. The Storm Atlas hook will pick this up\nif the Hive configuration is available in the Storm topology jar that is submitted on\nthe client and the cluster name is defined there. This happens similarly for HBase\ndata sets. In case this configuration is not available, the cluster name set in the Storm\nconfiguration will be used.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`atlas.cluster.name: \"cluster_name\"\n`}</code></pre>\n    <p>{`In `}<em parentName=\"p\">{`$STORM_HOME/conf/storm_env.ini`}</em>{`, set an environment variable as follows:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`STORM_JAR_JVM_OPTS:\"-Datlas.conf=$ATLAS_HOME/conf/\"\n`}</code></pre>\n    <p>{`where ATLAS_HOME is pointing to where ATLAS is installed.`}</p>\n    <p>{`You could also set this up programatically in Storm Config as:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-shell\"\n      }}>{`    Config stormConf = new Config();\n    ...\n    stormConf.put(Config.STORM_TOPOLOGY_SUBMISSION_NOTIFIER_PLUGIN,\n            org.apache.atlas.storm.hook.StormAtlasHook.class.getName());\n`}</code></pre>\n    </MDXLayout>;\n}\n\nMDXContent.isMDXComponent = true;\n  "],"sourceRoot":""}